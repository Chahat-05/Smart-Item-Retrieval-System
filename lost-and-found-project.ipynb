{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9947177,"sourceType":"datasetVersion","datasetId":6116693}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Imported Libraries**\n\n### **1. Data Handling and Visualization**\n- **`numpy`**: For numerical computations.\n- **`os`**: For file and directory operations.\n- **`matplotlib.pyplot`**: For plotting graphs and visualizations.\n- **`seaborn`**: For creating enhanced data visualizations like heatmaps.\n\n### **2. Deep Learning Frameworks**\n- **TensorFlow/Keras**:\n  - **`ImageDataGenerator`**: For augmenting and preprocessing images.\n  - **`MobileNet`**: A pre-trained MobileNet model for transfer learning.\n  - **`Dense`, `Dropout`, `Sequential`**: For building custom neural network layers.\n  - **`Adamax`**: An optimizer for training the model.\n  - **Callbacks**:\n    - `EarlyStopping`: Stops training when performance stops improving.\n    - `ModelCheckpoint`: Saves the model at checkpoints during training.\n\n- **PyTorch**:\n  - **`torch`** and **`torch.nn`**: For creating and training neural networks.\n  - **`torchvision.transforms`** and **`models`**: For preprocessing and using pre-trained models.\n\n### **3. Image Processing**\n- **`PIL.Image`**: For handling image operations in PyTorch.\n- **`load_img`, `img_to_array`**: For loading and converting images in TensorFlow/Keras.\n\n### **4. Evaluation Metrics**\n- **`classification_report`, `confusion_matrix`**: For analyzing model performance.\n- **`roc_curve`, `auc`**: For plotting the Receiver Operating Characteristic (ROC) curve and calculating Area Under Curve (AUC).\n- **`ConfusionMatrixDisplay`**: For visualizing confusion matrices.\n\n---\n\n## **Functionality**\n\n### **1. Directory Listing**\n- **`os.listdir('/kaggle/input/')`**:\n  - Lists files and folders in the `/kaggle/input/` directory, which is typically used for accessing datasets in a Kaggle notebook.\n\n### **2. Model Setup**\n- Leverages **`MobileNet`** from TensorFlow for transfer learning to create a powerful image classification model.\n- Includes preparation for using PyTorch as an alternative framework for building and training models.\n\n### **3. Metrics and Visualizations**\n- Implements confusion matrix, classification report, and ROC curve for a detailed evaluation of the model's performance.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adamax\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nos.listdir('/kaggle/input/')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explanation of the Data Preprocessing and Generators\n\nSetting up data preprocessing and generators to prepare the dataset for training, validation, and testing using **Keras' ImageDataGenerator**. Here's a breakdown of the steps:\n\n---\n\n## **1. Image Dimensions**\n- **`IMG_HEIGHT, IMG_WIDTH = 224, 224`**:\n  - Sets the target dimensions to which all images will be resized. This is commonly used for pre-trained models like MobileNet.\n\n---\n\n## **2. Data Augmentation**\n- **`ImageDataGenerator`**:\n  - A utility for real-time data augmentation during training to improve model generalization.\n\n### **Train Data Generator (`train_datagen`)**\n- **Rescaling**: Normalizes pixel values to the range `[0, 1]` by dividing by 255.\n- **Augmentations**:\n  - `rotation_range=20`: Randomly rotates images within a range of 20 degrees.\n  - `width_shift_range=0.25`, `height_shift_range=0.25`: Shifts the image horizontally and vertically by up to 25%.\n  - `zoom_range=0.2`: Randomly zooms images in and out by 20%.\n  - `horizontal_flip=True`, `vertical_flip=True`: Flips images horizontally and vertically.\n  - `fill_mode='nearest'`: Fills empty pixels caused by augmentations using the nearest pixel value.\n- **Validation Split**: Splits the dataset into 80% training and 20% validation.\n\n### **Validation and Test Data Generators**\n- **Validation (`val_datagen`)** and **Test (`test_datagen`)**:\n  - Rescales images to normalize pixel values but do not apply augmentations since these datasets are used for evaluation.\n\n---\n\n## **3. Data Generators**\n- **`flow_from_directory`**:\n  - Loads images directly from directories and applies transformations on-the-fly.\n\n### **Training Generator (`train_generator`)**\n- Directory: `/kaggle/input/ml-project-dataset/ml_project/train/train`\n- **Parameters**:\n  - `target_size=(224, 224)`: Resizes images to match the dimensions required by the model.\n  - `batch_size=32`: Specifies the number of images per batch.\n  - `class_mode='categorical'`: Indicates multi-class classification.\n  - `subset='training'`: Uses 80% of the dataset as training data.\n\n### **Validation Generator (`val_generator`)**\n- Directory: Same as training (`/kaggle/input/ml-project-dataset/ml_project/train/train`).\n- **Parameters**:\n  - Same as `train_generator` but uses `subset='validation'` to select the validation split.\n  - `shuffle=True`: Ensures the validation data is shuffled.\n\n### **Test Generator (`test_generator`)**\n- Directory: `/kaggle/input/ml-project-dataset/ml_project/test/test`\n- **Parameters**:\n  - `shuffle=False`: Ensures the test data is not shuffled for consistent evaluation.\n\n---\n\n## **Summary**\n- Preprocesses and augments training data for better model generalization.\n- Sets up separate generators for training, validation, and testing.\n- Ensures the data is correctly split and preprocessed for a smooth training pipeline.\n","metadata":{}},{"cell_type":"code","source":"IMG_HEIGHT , IMG_WIDTH = 224, 224\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.25,\n    height_shift_range=0.25,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest',\n    validation_split = 0.2\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/lost-and-found-project-dataset/ml_project/train',\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=32,\n    class_mode='categorical',\n    subset = 'training'\n)\n\nval_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/lost-and-found-project-dataset/ml_project/train',\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=32,\n    class_mode='categorical',\n    subset = 'validation',\n    shuffle = True\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    '/kaggle/input/lost-and-found-project-dataset/ml_project/test',\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=32,\n    class_mode='categorical',\n    shuffle=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explanation of the Model Definition\n\nDefining a deep learning model for multi-class image classification using **MobileNet** as the base model and additional custom layers for classification.\n\n---\n\n## **1. Number of Classes**\n- **`num_classes = 11`**:\n  - Indicates the number of output classes in the classification task.\n\n---\n\n## **2. Base Model**\n- **`base_model = MobileNet(...)`**:\n  - **MobileNet**: A pre-trained model loaded with `ImageNet` weights.\n  - **Parameters**:\n    - `weights='imagenet'`: Uses pre-trained weights from the ImageNet dataset.\n    - `include_top=False`: Removes the top classification layer to add custom layers.\n    - `pooling='avg'`: Applies global average pooling to the feature maps.\n    - `input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)`: Specifies the input image dimensions.\n\n- **Freezing Layers**:\n  - **`for layer in base_model.layers[:-10]: layer.trainable = False`**:\n    - Freezes all but the last 10 layers of MobileNet to prevent overfitting while fine-tuning the model on the new dataset.\n\n---\n\n## **3. Custom Model Architecture**\n- **`Sequential()`**: A sequential model is used to stack layers linearly.\n\n### Added Layers:\n1. **Base Model**: MobileNet as the feature extractor.\n2. **Dense Layer (256)**:\n   - Adds a fully connected layer with 256 neurons and ReLU activation.\n   - **Dropout (0.45)**: Reduces overfitting by randomly disabling 45% of neurons during training.\n3. **Dense Layer (128)**:\n   - Adds another fully connected layer with 128 neurons and ReLU activation.\n   - **Dropout (0.3)**: Reduces overfitting by disabling 30% of neurons.\n4. **Dense Layer (64)**:\n   - Fully connected layer with 64 neurons and ReLU activation.\n5. **Output Layer**:\n   - **`Dense(num_classes, activation='softmax')`**:\n     - Outputs probabilities for each of the 11 classes using the softmax activation function.\n\n---\n\n## **4. Compilation**\n- **`model.compile(...)`**:\n  - **Optimizer**: `Adamax` with a learning rate of 0.001.\n  - **Loss Function**: `categorical_crossentropy` for multi-class classification.\n  - **Metrics**: Tracks `accuracy` during training and evaluation.\n\n---\n\n## **5. Model Check**\n- **Dummy Input**:\n  - **`dummy_input = np.random.random((1, IMG_HEIGHT, IMG_WIDTH, 3))`**:\n    - Generates a random input tensor to ensure the model is built correctly.\n  - **`model(dummy_input)`**: Verifies that the model can process the input without errors.\n\n- **Model Summary**:\n  - **`model.summary()`**:\n    - Displays the architecture, trainable parameters, and output shapes.\n\n---\n\n## **Summary**\nThis code builds a transfer learning model using MobileNet as a feature extractor with additional custom dense layers for classification. It uses dropout layers to prevent overfitting and compiles the model with an Adamax optimizer and categorical cross-entropy loss for multi-class classification.\n","metadata":{}},{"cell_type":"code","source":"num_classes = 11\n\nbase_model = MobileNet(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMG_HEIGHT , IMG_WIDTH, 3))\n\nfor layer in base_model.layers[:-10]:\n    layer.trainable = False\n\nmodel = Sequential()\n\nmodel.add(base_model)\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.45))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(64, activation='relu'))\n\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(optimizer=Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\ndummy_input = np.random.random((1, IMG_HEIGHT, IMG_WIDTH, 3))\nmodel(dummy_input)\n\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Explanation of the Training Setup and Callbacks\n\nThis section defines the training process for the model using **early stopping** and **model checkpointing** to improve training efficiency and save the best-performing model.\n\n---\n\n## **1. Early Stopping**\n- **`EarlyStopping(...)`**:\n  - A callback to stop training when the validation performance stops improving.\n  - **Parameters**:\n    - `monitor='val_loss'`: Monitors the validation loss during training.\n    - `patience=5`: Stops training if the validation loss does not improve for 5 consecutive epochs.\n    - `restore_best_weights=True`: Restores the model weights to the best-performing epoch after training stops.\n\n---\n\n## **2. Model Checkpoint**\n- **`ModelCheckpoint(...)`**:\n  - A callback to save the model at checkpoints during training.\n  - **Parameters**:\n    - `'best_model.keras'`: Saves the model as a file named `best_model.keras`.\n    - `save_best_only=True`: Ensures only the best model (with the lowest validation loss) is saved.\n    - `monitor='val_loss'`: Saves the model based on validation loss.\n\n---\n\n## **3. Training the Model**\n- **`model.fit(...)`**:\n  - Trains the model using the preprocessed training and validation data.\n  - **Parameters**:\n    - `train_generator`: The training dataset generator with augmented images.\n    - `validation_data=val_generator`: The validation dataset generator.\n    - `epochs=20`: Specifies a maximum of 20 epochs for training.\n    - `callbacks=[early_stopping, model_checkpoint]`: Includes the callbacks for early stopping and saving the best model.\n\n---\n\n## **4. History Object**\n- The **`history`** object stores training details such as:\n  - Training and validation loss.\n  - Training and validation accuracy.\n- This object can be used to visualize the training progress and performance metrics.\n\n---\n\n## **Summary**\nThis code sets up the training process with:\n1. Early stopping to prevent overfitting and save training time.\n2. Model checkpointing to save the best-performing model.\n3. Training and validation data generators to monitor performance.\n","metadata":{}},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n\nhistory = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=20,\n    callbacks=[early_stopping, model_checkpoint],\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explanation of Model Evaluation on Test Data\n\nThis section evaluates the trained model on the test dataset to measure its performance on unseen data.\n\n---\n\n## **1. Loading Best Model Weights**\n- **`model.load_weights('best_model.keras')`**:\n  - Loads the weights of the model saved during training.\n  - These weights correspond to the best-performing model based on validation loss (as monitored by the `ModelCheckpoint` callback).\n\n---\n\n## **2. Evaluating on Test Data**\n- **`model.evaluate(test_generator)`**:\n  - Evaluates the model's performance on the test dataset.\n  - **Parameters**:\n    - `test_generator`: The test dataset generator prepared earlier.\n  - **Returns**:\n    - `test_loss`: The loss on the test dataset, calculated using the loss function (`categorical_crossentropy`).\n    - `test_acc`: The accuracy on the test dataset, as specified in the `metrics` parameter during compilation.\n\n---\n\n## **3. Printing Test Accuracy**\n- **`print(f\"Test accuracy: {test_acc:.2f}\")`**:\n  - Displays the test accuracy in a formatted manner with two decimal places.\n\n---\n\n## **Summary**\nThis code:\n1. Loads the best model weights saved during training.\n2. Evaluates the model on unseen test data to compute the test loss and accuracy.\n3. Prints the test accuracy as a measure of the model's generalization capability.","metadata":{}},{"cell_type":"code","source":"model.load_weights('best_model.keras')\ntest_loss, test_acc = model.evaluate(test_generator)\nprint(f\"Test accuracy: {test_acc:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explanation of Model Prediction and Evaluation\n\nThis section performs predictions on the test data, converts the predictions to class labels, and evaluates the model's performance using the classification report.\n\n---\n\n## **1. Predicting on Test Data**\n- **`y_pred = model.predict(test_generator)`**:\n  - Uses the trained model to predict the class probabilities for each image in the test dataset.\n  - **`test_generator`**: A generator that provides batches of test images for the model to predict.\n\n---\n\n## **2. Converting Predictions to Class Labels**\n- **`y_pred_classes = np.argmax(y_pred, axis=1)`**:\n  - **`np.argmax()`**: Converts the predicted probabilities into class labels by selecting the index of the highest probability in each prediction.\n  - **`axis=1`**: Operates along the class axis, i.e., for each prediction (each image), it selects the class with the highest probability.\n\n---\n\n## **3. Extracting True Labels**\n- **`y_true = test_generator.classes`**:\n  - Extracts the true class labels from the test data generator.\n  - **`test_generator.classes`**: Contains the true class labels of all images in the test set.\n\n---\n\n## **4. Generating Classification Report**\n- **`classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys())`**:\n  - Generates a classification report that includes precision, recall, f1-score, and support for each class.\n  - **`y_true`**: The true labels of the test set.\n  - **`y_pred_classes`**: The predicted class labels.\n  - **`target_names=test_generator.class_indices.keys()`**: Maps class indices to class labels, so the report displays the actual class names instead of numerical labels.\n\n---\n\n## **Summary**\nThis code:\n1. Uses the trained model to predict the class probabilities for the test data.\n2. Converts the predicted probabilities into class labels.\n3. Generates and prints a detailed classification report that evaluates the model's performance across each class based on precision, recall, and F1-score.\n","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(test_generator)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\ny_true = test_generator.classes\n\nprint(classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_query_image(image_path, target_size):\n    image = load_img(image_path, target_size=target_size) \n    image_array = img_to_array(image)                   \n    image_array = np.expand_dims(image_array, axis=0)   \n    image_array = image_array / 255.0                   \n    return image_array\n\nquery_image_path = \"\"\n\nquery_image = preprocess_query_image(query_image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n\npredictions = model.predict(query_image)\npredicted_class_index = np.argmax(predictions, axis=1)[0]\n\nclass_indices = train_generator.class_indices\nclass_labels = {v: k for k, v in class_indices.items()}\nquery_image_category = class_labels[predicted_class_index]\n\nprint(f\"Predicted category for the query image: {query_image_category}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explanation of Siamese Network Model for Image Similarity\n\nThis code defines a **Siamese Network** to find the most similar image to a given query image by comparing feature embeddings. The model uses a pre-trained ResNet-18 model for feature extraction, then performs image similarity based on cosine similarity.\n\n---\n\n## **1. Siamese Network Class**\n- **`class SiameseNetwork(nn.Module):`**:\n  - Defines the Siamese network architecture using PyTorch's `nn.Module`.\n  - The network utilizes a pre-trained **ResNet-18** model, excluding the final classification layer, to extract image features.\n- **`self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])`**:\n  - The `base_model` is a ResNet-18 pre-trained on ImageNet.\n  - We remove the final layer to only keep the feature extraction part of the model.\n  \n---\n\n## **2. Forward Pass**\n- **`def forward(self, x):`**:\n  - Defines the forward pass for the network, which extracts features from the input image.\n  - The output is reshaped using `.view()` to a flat vector of features.\n\n---\n\n## **3. Image Transformations**\n- **`transform = transforms.Compose([...])`**:\n  - Applies a series of transformations to the input image:\n    - **Resize** the image to 224x224 pixels.\n    - **ToTensor** converts the image to a tensor.\n    - **Normalize** adjusts the pixel values based on the ImageNet mean and standard deviation.\n\n---\n\n## **4. Model Initialization**\n- **`device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`**:\n  - Automatically detects whether a GPU is available and sets the device accordingly (either CUDA or CPU).\n- **`model = SiameseNetwork().to(device)`**:\n  - Initializes the Siamese network and moves it to the appropriate device (GPU/CPU).\n- **`model.eval()`**:\n  - Sets the model to evaluation mode (no gradient updates).\n\n---\n\n## **5. Get Image Embedding**\n- **`def get_embedding(image_path, model):`**:\n  - Loads an image from the specified path, applies the transformations, and computes the feature embedding using the Siamese network.\n  - **`embedding = model(image_tensor).cpu().numpy()`**: The embedding is obtained by passing the image tensor through the model.\n\n---\n\n## **6. Finding the Most Similar Image**\n- **`def find_most_similar(query_image_path, folder_path, model):`**:\n  - Given a query image, this function finds the most similar image from a folder using cosine similarity between their embeddings.\n  - The similarity score is computed as:\n    - **`similarity = np.dot(query_embedding, folder_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(folder_embedding))`**.\n  - If the similarity score is greater than 0.80, it considers that image as a potential match.\n\n---\n\n\n## **Summary**\nThis code performs image similarity comparison using a Siamese network with a ResNet-18 backbone. It extracts feature embeddings from images, computes the cosine similarity between the query image and others in the dataset, and identifies the most similar image.\n","metadata":{}},{"cell_type":"code","source":"class SiameseNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        base_model = models.resnet18(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        return features.view(features.size(0), -1)\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SiameseNetwork().to(device)\nmodel.eval()\n\ndef get_embedding(image_path, model):\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = transform(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        embedding = model(image_tensor).cpu().numpy()\n    return embedding.flatten()\n\ndef find_most_similar(query_image_path, folder_path, model):\n    query_embedding = get_embedding(query_image_path, model)\n    max_similarity = -1\n    most_similar_image = None\n\n    for file in os.listdir(folder_path):\n        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n            image_path = os.path.join(folder_path, file)\n            folder_embedding = get_embedding(image_path, model)\n            similarity = np.dot(query_embedding, folder_embedding) / (\n                np.linalg.norm(query_embedding) * np.linalg.norm(folder_embedding)\n            )\n\n            if similarity > max_similarity and similarity > 0.80:\n                max_similarity = similarity\n                most_similar_image = image_path\n\n    return most_similar_image, max_similarity\n\nif __name__ == \"__main__\":\n    query_image = \"\"\n    images_folder = f\"{query_image_category}\"\n    \n    similar_image, similarity_score = find_most_similar(query_image, images_folder, model)\n    print(f\"Most similar image: {similar_image} with similarity score: {similarity_score:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef display_images_side_by_side(query_image_path, similar_image_path):\n    query_image = Image.open(query_image_path).convert(\"RGB\")\n    similar_image = Image.open(similar_image_path).convert(\"RGB\")\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    \n    axes[0].imshow(query_image)\n    axes[0].axis(\"off\")\n    axes[0].set_title(\"Query Image\")\n    \n    axes[1].imshow(similar_image)\n    axes[1].axis(\"off\")\n    axes[1].set_title(\"Most Similar Image\")\n    \n    plt.tight_layout()\n    plt.show()\n    \nif similar_image:\n    display_images_side_by_side(query_image, similar_image)\nelse:\n    print(\"No similar images found in the folder.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}